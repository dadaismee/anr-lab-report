---
bibliography: service/41_vashchenko_tm.json
---
# Сравнительный анализ актуальных подходов к анализу неструктурированной текстовой информации (стохастический блокмоделинг, LDA и BERT модели) на примере анализе дискурсов в социальных медиа

## Введение
Повсеместное использование социальных сетей в качестве средства коммуникации подчеркивает необходимость внимания к особенностям методологии и инструментария, необходимых для анализа данных, производимых пользователями социальных сетей. В частности, актуализируется вопрос обработки коротких текстов, недостаток семантического контекста в которых может стать серьезным ограничением для работы многих алгоритмов машинного обучения, применяемых к текстовым данным.

Одной из особенностей текстовых данных, производимых в социальных сетях, является отсутствие «разметки» - не существует истинных меток тематической принадлежности, тональной нагрузки и прочих характеристик содержания текстового источника. В таких случаях повышается релевантность методов машинного обучения «без учителя» (unsupervised), способных идентифицировать сходные группы внутри данных. Тематическое моделирование – направление в рамках дисциплины обработки естественных языков (NLP), направленное на агрегацию текстов в тематические кластеры. Практические приложения методов тематического моделирования кросс-дисциплинарны и  включают рекомендательные системы [@godin2013], наукометрию [@asmussen2019],  дискурсивные исследования [@lyu2021; @hoseini2023], автоматической идентификации событий в новостях и социальных сетях [@hu2012; @qian2015; @gong2017; @lyu2021] и проч. Тем не менее, предпосылки о данных, на которых основаны классические алгоритмы тематического моделирования, не всегда выполняются для текстов сообщений в социальных сетях. Короткая длина текстов, мотивируемая платформенными лимитами на объём публикаций и комментариев, не предоставляет достаточного семантического контекста для анализа слов классическими методами [@hong2010; @albalawi2020; @quiang2022].

Следовательно, необходимо формализованное сравнение качества разных алгоритмов тематического моделирования для коротких текстов с целью обнаружения наиболее эффективного с вычислительной и интерпретационной точки зрения подхода. В отличие от большинства обзорных исследований по теме, мы фокусируемся не на анализе вариаций латентного распределения Дирихле (LDA), а предлагаем сравнить его работу с методами, основанными на стохастическом блокмоделировании и векторных представлениях слов в модели типа «трансформер», соответственно.

Разнообразие сравниваемых методов позволяет более конкретно изучить связь между устройством подхода к тематическому моделированию и особенностями результирующих тематических кластеров, а также отметить значимые направления потенциального развития и оптимизации наличествующих инструментов для задач анализа коротких текстов.

## Цели исследования
Провести формальную оценку качества алгоритмов тематического моделирования на данных коротких текстов на примере комментариев в социальной сети TikTok. Оценить плюсы и недочеты разных инструментов тематического моделирования в приложении к коротким текстам, производимых в социальных сетях.

## Практическая значимость исследования
Настоящее исследование призвано проанализировать эффективность алгоритмов, относящихся к разным подходам в тематическом моделировании, на нетипичном наборе данных: в отличие от большинства датасетов, зачастую используемых для оценки методов тематического моделирования, комментарии в социальных сетях содержат большое количество опечаток и ошибок, сокращений, неконвенциональной лексики (например, сленга), при этом имея крайне короткую длину. В то же время, понимание перспектив и ограничений использования методов тематического моделирования именно для данных социальных сетей может стать важным инструментом для исследований медиа-коммуникации разной дисциплинарной и тематической направленности.

## Обзор современных методов тематического моделирования
Методы тематического моделирования получили широкое распространение в конце 1990-х годов с появлением подхода PLSA [@hofmann2013] и его последующей адаптации, ставшей конвенцией в тематическом моделировании – LDA [@blei2003]. Методы тематического моделирования обычно основаны на некоторых предположениях о взаимосвязи между темами и документами. В частности, один из наиболее распространенных алгоритмов тематического моделирования LDA предполагает, что каждый документ представляет собой смесь тем, а каждая тема - распределение Дирихле по словам. Распределение Дирихле является многомерной формой бета-распределения, которое используется для генерации распределений для каждого слова в наборе данных. В целом генеративная тема модели смешения Дирихле-Мультиномиального распределения имеет такую структуру:

Для каждой темы $k ∈{1,…,K}$ 
          $ϕ_k  ~ Dir(β)$
$θ ~ Dir(α)$

Для каждого документа $m ∈{1,…,M}$
          $z_m ~ Mult(1,0)$ 
         и для каждого слова $n ∈{1,…,N_m }$
                 $x_mn~Mult(1,ϕ_(z_m ))$, где
                 
K - количество тем
M - количество документов в датасете
N - количество слов в документе

Хотя модели такого типа продемонстрировали высокую эффективность и качество для анализа художественной и научной литературы, что сделало их стандартом в отрасли, их применение для коротких текстов, доминирующих в социальных сетях, ограничено недостаточностью семантического контекста в коротких текстах, а также спорностью предпосылки наличия распределения тем внутри документа поскольку многие из них содержат только одну тему [@ahuja2015; @quiang2022].

Решение этой проблемы предлагают [@dieng2020], которые считают, что классическая LDA может достигать более высоких результатов в задачах тематического моделирования за счет комбинации с эмбеддингами слов (авторы используют word2vec skip-gram, но технических ограничений на то, как могут выглядеть векторные представления слов, нет). Предложенная ими модель ETM моделирует каждое слово категориальным распределением, естественным параметром которого является произведение между эмбеддингом слова и эмбеддингом назначенной ему темы, что позволяет сохранять и использовать больше семантической информации о словах в документах. Возможность ссылаться на общий контекст слова очень важна для эффективной обработки коротких документов, поскольку семантический контекст, который может быть выведен из самого набора данных, может быть искажен. Поэтому для получения более качественных кластеров слов наряду с традиционным LDA можно использовать предварительно обученные эмбеддинги.

Среди прочего, короткие текстовые данные предлагается обогащать метаданными, тэгами авторов [@rosen-zvi2012] и хэштегами [@wang2016] использовать длинные тексты для обучения модели тематического моделирования для инференса на коротких текстах [@phan2008] и проч. Эти адаптации, однако, не нивелируют проблему отсутствия критерия выбора количества тем (хотя в этом направлении также ведется работа, см. [@koltcov2021]) и обоснования предпосылки о соответствии распределения тем и слов внутри темы распределению Дирихле [@gerlach2018].

Вместо семантического обогащения модели @gerlach2018 предлагают структурное изменение, подходя к задаче тематического моделирования как к задаче выделения сообществ в сетях. Они предлагают метод иерархического стохастического блокмоделирования (hSBM) как более эффективный и адаптируемый алгоритм для выделения тем. Стохастические блочные модели – это статистический фреймворк, используемый для моделирования сложных сетей. В этой модели узлы сети разбиваются на различные группы или блоки, при этом вероятность появления ребер между узлами зависит от принадлежности узлов к той или иной группе. В модели предполагается, что узлы одной группы имеют схожий характер связности, а узлы разных групп - разный характер связности. Вероятность возникновения ребра между двумя вершинами определяется принадлежностью их к кластерам, причем вероятность возникновения ребра между вершинами одного кластера выше, чем между вершинами из разных кластеров. В основе hSBM лежит идея замены неинформативных приоров, используемых в LDA, на непараметрический подход: глубокую байесовскую иерархию приоров и гиперприоров, которые не делают предположений о свойствах данных более высокого порядка. Таким образом, авторы делают вывод о том, что hSBM могут стать решением вышеупомянутых недостатков традиционного LDA. Более того, они подтверждают свое утверждение, сравнивая производительность своей модели с LDA на большом наборе данных статей Википедии, а также искусственно созданных текстовых наборов данных, не соответствующих основным предположениям LDA, где hSBM значительно превосходит LDA (Там же).

Похожая логика применения иерархичности в выделении групп для сохранения и отображения гетерогенности в данных применяется и в наработках среди моделей тематического моделирования, ориентированных на расширение семантического контекста анализируемых документов. Например, методы, использующие представления слов, создаваемые трансформерными моделями, прибегают к иерархической кластеризации эмбеддингов как способу выделения тем. @grootendorst2022 описывает процедуру выделения тем при помощи трансформеров следующим образом:

1.         Для создания эмбеддингов документов используется предварительно обученная языковая модель на основе трансформера;

2.         Для учета недостатков высокой размерности вкраплений представления документов уменьшаются с помощью Uniform Manifold Approximation and Projection (UMAP) - инструмента для уменьшения размерности, превосходящего t-SNE и PCA за счет более высокой скорости сохранения локальных и глобальных особенностей данных [@mcinnes2020];

3.         Редуцированные вкрапления кластеризуются с помощью HDBSCAN - иерархического расширения классического DBSCAN, позволяющего выявлять кластеры различной плотности. При мягкой кластеризации шум помечается как выбросы, которым не присваивается кластерная метка, чтобы избежать включения несвязанных данных в какую-либо из тем;

4.         Для определения важности слов по кластерам, а не по документам, используется классовый TF-IDF, который позволяет получить распределения слов по темам для всех тематических кластеров. Векторы c-TF-IDF нормируются с помощью L1-нормы для сглаживания представлений тем.

Описанные особенности модели делают BERTopic высокопотенциальным подходом к тематическому моделированию, поскольку представления тем являются динамичными и гибкими, а также восприимчивыми к временным изменениям текстовых данных.

Так, развитие области тематического моделирования отмечается расширением спектра типов данных, с которыми алгоритмы выделения тематических групп способны эффективно работать. Хотя наиболее популярным аналитическим направлением стало обогащение контекста коротких текстов за счет предварительно обученных вкраплений и других типов данных, подключаемых к модели, были сделаны шаги и в сторону изменения концептуальных основ подхода к задаче тематического моделирования, одним из которых является использование тематических кластеров, произведенных методами сетевого анализа.

## Методология и дизайн исследования
Целью настоящего исследования было сравнение качества алгоритмов тематического моделирования для анализа коротких текстов, представленных на примере комментариев и хэштегов в социальной сети TikTok. Мы прибегаем к сравнению алгоритмов, относящимся к разным группам методов. Опираясь на категоризацию, предложенную [@abdelrazek2022], мы используем модели из двух разных из выделенных ими групп: вероятностные (LDA и SBMTM), нейросетевые (ETM и BERTopic). 

LDA обогащается предобученными эмбеддингами GloVe (Global Vectors) для русского языка Navec^[https://natasha.github.io/navec/?ysclid=l9kgcwq6eu573989592]. Выбор обоснован малым объёмом памяти, необходимой для их использования и дообучения при большом объёме словаря: используемые эмбеддинги hudlit_12B_500K_300d_100q покрывают 98% слов в художественных текстах, занимая 95.8 Мб RAM. GloVe основан на глобальном статистическом подходе: целью обучения является минимизация разницы между точечным произведением векторов слов и логарифмом вероятностей соприсутствия в корпусе на основе матрицы соприсутствия. Использование глобальных статистик совстречаемости слов позволяет определить сравнительную значимость слов в тексте [@pennington2014].

Для триангуляции оценки качества, описанные алгоритмы сравниваются при помощи следующих метрик:

·      Время, затрачиваемое на вычисления;

·      Когерентность (связность) выделяемых тем

Оценка связности производится при помощи метрик NPMI и UMass на топ-10 словах каждой темы: эти метрики рассчитывают близость слов в теме на основе паттернов их совместного появления в корпусе. NPMI хорошо коррелирует с человеческой оценкой [@aletras2013], однако UMass более устойчива к вариациям в референтном корпусе и шуму, а также, в отличие от NPMI, не агностична к позиции слова при анализе: значение метрики будет меняться в зависимости от того, какое слово является центральным, а какое – контекстным при расчёте совстречаемости.

·      Разнообразие тем

Мы используем метрику, обратную Rank-Biased Overlap (RBO) – рекурсивно оценивающую долю пересечений между ранжированными по значимости списками слов в теме на разных уровнях «глубины» погружения в список. Низкая доля подобных пересечений между выделяемыми темами указывает на их высокое разнообразие.

## Эмпирическая база исследования
Для задачи оценки качества на неразмеченном датасете мы используем массив русскоязычных комментариев к урбанистическим видео в TikTok. Выбор TikTok в качестве платформы для анализа обусловлен устойчивой популярностью сети до блокировки в России, а также адаптацией формата в прочих социальных сетях, до сих пор доступных на территории РФ (YouTube, ВКонтакте). Фокус на урбанистической тематике основан на потребности в общей тематической когерентности анализируемых видео, которую можно использовать в качестве базового бенчмарка тематики. Сама по себе тема урбанистики удобна своей популярностью, предоставляющей достаточно объёмную базу для исследования, а также способностью вовлекать аудиторию в дискуссию за счёт близости темы зрителям, что обеспечивает наличие содержательных обсуждений в комментариях.

Отбор видео осуществлялся по хэштегам, список которых расширялся при помощи рекомендательного алгоритма платформы. Финальный набор содержит 17 позиций. Из массива видео, относящихся к этим хэштегам, доступными оказались 2625 видео и содержащими комментарии - 1271.

|   |   |   |
|---|---|---|
|Хэштег|Кол-во видео|Кол-во просмотров|
|человейники|368|8000000|
|метроспб|292|84600000|
|московскоеметро|249|89900000|
|городскаясреда|243|13800000|
|транспорт|214|448600000|
|урбанизм|207|36600000|
|реновация|198|21400000|
|благоустройство|189|116300000|
|монорельс|183|4100000|
|общественныйтранспорт|179|77800000|
|урбанистика|170|74900000|
|часпик|93|111600000|
|комфортнаясреда|87|456800|
|урбанизация|46|614800|
|транспортная реформа|33|183000|
|безбарьернаясреда|21|1200000|
|городскоепланирование|5|381000|

Выгрузка комментариев производилась с помощью Selenium WebDriver для Chrome и языков программирования JavaScript и Python.

Тексты комментариев были предобработаны при помощи приведения всех слов-токенов к нормальной форме, удалением стоп-слов и фильтрацией токенов, встречающихся менее 10 раз или в более, чем 80% документов. В анализ включались единичные токены, а также биграммы. Для предобработки использовались библиотеки NLTK, pymorphy2 и Gensim для Python.

## Результаты
Мы обнаруживаем, что, с точки зрения связности результирующих тематических кластеров, ни одна из используемых моделей не демонстрирует результатов высокого качества, для большинства из них качество можно назвать средним (Табл. №2). Если по метрике NPMI результаты работы алгоритмов сходны, то по метрике связности UMass расхождения велики: SBMTM демонстрирует более низкое качество, что указывает на более низкое качество ранжирования слов внутри темы и недостаток точности при агрегации слов в кластеры. Чем выше уровень иерархии, тем ниже значение UMass – при объединении тем их связность падает. Опираясь на диапазоны значений NPMI, мы также можем заметить, что качество тем сильно различается внутри каждой из моделей: они содержат как темы с низкой, так и с более высокой когерентностью.

|   |   |   |   |   |
|---|---|---|---|---|
||SBMTM|SBMTM|ETM|BERTopic|
||Level 1|Level 2|Navec|
|Время на вычисления|244|244|128|46|
|Количество тем|11|2|60|19|
|NPMI (диапазон значений)|[-0.19;<br><br>0.21]|[-0.05;<br><br>0.03]|[-0.117; 0.311]|[-0,05;<br><br>0,339]|
|Средний NPMI по темам|0.05|-0.01|0.042|0.12|
|Средний UMass по темам|-7.66|-8.7|-3.96|-0.23|

Более того, следует обратить внимание на то, что разброс значений когерентности тем у SBMTM выше, чем у BERTopic: если последний инструмент склонен создавать темы примерно одного и того же среднего уровня качества, то SBMTM производит большое количество как очень низко, так и очень высоко когерентных тем.

![Распределение когерентности тем BERTopic и SBMTM](41_1.png)

С точки зрения разнообразия тем ETM уступает BERTopic (для SBMTM метод, производящий пересекающиеся сообщества, не имплементирован): в составе тем, производимых ЕТМ, присутствует много повторяющихся высокочастотных слов (например, «человек», «общество», «город»), в то время как BERTopic производит более уникальные по составу темы. Результаты работы ETM далее не интерпретируются ввиду низкой уникальности тем и плохой различительной способности слов, входящих в итоговые темы.

BERTopic значительно превосходит свои альтернативы по всем выбранным критериям качества модели - он самый быстрый и выдает наиболее интерпретируемые и понятные темы по сравнению с конкурентами. Такая существенная разница в качестве может быть частично объяснена контекстуальными эмбеддингами, используемыми в BERT, то есть эмбеддинги слов различаются от предложения к предложению в зависимости от их контекста, а также структурного положения в предложении. Такой подход может быть наиболее полезен для тематического моделирования коротких текстов, поскольку статические эмбеддинги могут не распознать устойчивые паттерны соприсутствия или семантической связи из-за малой длины документа и, соответственно, небольшого размера окна для обучения.

В целом, в соответствии с утверждениями автора оригинальной статьи, BERTopic, как с редукцией тем, так и без нее, создает хорошо интерпретируемые темы. Однако эти темы чрезвычайно специфичны и контекстуальны. Другими словами, BERTopic выбирает узкие темы, а не более широкие, и чувствителен к локальным паттернам дискурсивной активности.

![Топ-5 слов для выделенных BERTopic тем](41_2.png)

Для SBMTM на первом уровне агломерации модель выделила 11 тем, которые представлены в таблице №3 вместе с 10 наиболее частотными примерами слов каждой из них. Помимо тематических тем, модель также группировала слова по их языку, если он не русский, и по структурной позиции в предложении. Так, темы 10 и 11 представляют английские и украинские слова, а тема 3 состоит из слов речевых актов, таких как приветствия, поздравления, благодарности и однословных экспрессивных высказываний: интересно ('интересно'), круто ('топ', 'здорово'). Остальные темы сгруппированы в зависимости от предмета обсуждения.

Темы 1 и 5 трудно интерпретировать однозначно, так как они состоят из разных слов, обозначающих городскую жизнь, где есть упоминания и о транспорте, и о работе, и о деньгах, и т.д. Тема 2 может быть интерпретирована как единичное высказывание, так как все наиболее часто встречающиеся слова в этой теме относятся к одной и той же идее - жители Москвы сообщают автору публикации о том, что, несмотря на проживание в Москве, они не знают о чем-то, что упоминается в видеоролике. Тема 4 посвящена общественному транспорту в целом и, в частности, в Санкт-Петербурге.

Темы 6-9, однако, более тематически целостны и, следовательно, могут быть интерпретированы однозначно. Тема 6 касается конкретного вопроса пользования общественным транспортом - уступать ли свое место другому пассажиру (в частности, женщинам, детям и пожилым людям). Примечательно, что слово, обозначающее пожилую женщину, носит сатирический характер: вместо "бабушка" используется "бабка", что считается неуважительным описанием, и подчеркивает, что вопрос вызывает споры и негативные настроения. Тема 7 включает в себя общеупотребительную лексику, связанную с использованием общественного транспорта. Наиболее употребительные слова этой темы отражают правила и ожидания людей, пользующихся общественным транспортом, а также слова, указывающие на возможные проблемы и исключения. Например, слово "сторона" (side) часто используется в наборе данных наряду со словом "лифт" для обсуждения вопроса о том, предпочтительно ли пользоваться обеими сторонами лифта или оставлять левую сторону открытой для желающих подняться по лестнице - весьма распространенная практика в Москве, однако не соответствующая правилам пользования метрополитеном. Другой пример - слово "коляска" (может означать детскую или инвалидную коляску), которое также может относиться к проблемам, с которыми сталкиваются маломобильные люди или родители при пользовании общественным транспортом. Тема 8 посвящена жилым домам, включая упоминания конкретных типов зданий, таких как "хрущёвка" (хрущёвка - тип советских жилых домов, массово выпускавшихся по плану обеспечения большинства населения доступным жильём), а также иронические названия высотных зданий ("человейник" - игра слов, сочетающая "человек" и "муравейник", чтобы подчеркнуть, насколько тесны некоторые из новых жилых комплексов). Тема 9 включает политизированную лексику, сравнивающую Россию с ее предполагаемыми политическими противниками.

|   |   |
|---|---|
|**Тема №**|**Топ-10 слов**|
|1|город, год, делать, время, деньги, работать, район, дорога, жизнь, хотеть|
|2|жить, Москва, знать, видеть, сделать, хороший, думать, видео, смотреть, первый|
|3|спасибо, привет, молодец, интересно, круто, жиза, топ, классно, поздравлять, здорово|
|4|метро, автобус, трамвай, ездить, станция, новый, Питер, поезд, ветка, ходить|
|5|человек, говорить, сказать, стоить, машина, работа, идти, проблема, понимать, дело|
|6|место, ехать, ребенок, уступать, сидеть, женщина, девушка, мужчина, бабка|
|7|стоять, дверь, сторона, остановка, выходить, правило, эскалатор, быстрый, переход, коляска|
|8|дом, строить, квартира, построить, купить, человейник, земля, покупать, хрущёвка, этаж|
|9|Россия, страна, Путин, бог, Европа, Украина, русский, слава, Америка, завидовать|
|10|You, the, that, and, not, just, now, what, peace, she|
|11|расти, укр, вонь, петров, Крым, але, така, буде, хто, немой|
Выше по иерархии тем группы второго уровня состоят только из двух кластеров, объединяющих описанные выше темы. На втором уровне иерархии тем большинство описанных выше тем объединяются в группу слов, относящихся к городской жизни, включающую как темы жилых домов, так и общественного транспорта. Примечательно, что среди наиболее частотных слов в этой теме встречается топоним - название столицы России, Москвы. Вторая тема, обособленная от урбанистических дискуссий, содержит политизированную лексику.

|   |   |
|---|---|
|**Тема №**|**Топ-10 слов**|
|1|жить, человек, город, метро, дом, год, Москва, место, автобус|
|2|Россия, страна, Путин, бог, Европа, Украина, русский, слава, завидовать|
## Обсуждение и выводы
Результаты анализа качества тематического моделирования с использованием метрик coherence и разнообразия тем показывают, что BERTopic превосходит альтернативные методы в создании когерентных тем. Преимущество этого алгоритма достигается благодаря использованию c-TF-IDF, которое является уникальным алгоритмом взвешивания слов по их значимости внутри темы и отсутствует у других методов. Однако, было обнаружено, что хотя темы, выделенные BERTopic, имеют в среднем более высокую когерентность, SBMTM на нулевом уровне иерархии создает больше тем с высокой когерентностью, что уравновешивается темами с низкими показателями качества при усреднении. Применение совместного моделирования сообществ для документов и слов в бимодальной сети позволяет SBMTM более эффективно кластеризировать документы, однако его темы более подвержены генерализации. Иерархическая структура SBMTM успешно вносит гетерогенность в производимые темы, в то время как BERTopic сконцентрирован на локальных семантических паттернах.

Проведенные эксперименты подтверждают перспективность использования альтернативных методов LDA для тематического моделирования на коротких текстах, особенно сетевого подхода к представлению текстовых данных. Мы находим, что иерархическое блокмоделирование превосходит методы, основанные на словарных эмбеддингах, по интерпретируемости и генерализации интерпретаций, что было продемонстрировано ранее в задаче классификации документов на англоязычных датасетах [@qiang2022]. Тем не менее, следует отметить, что SBMTM уступает альтернативам по производительности и качеству ранжирования слов в темах по значимости. Это подтверждается высоким разбросом значений и заметными различиями в распределении coherence в зависимости от числа анализируемых слов. Таким образом, сетевой анализ представляет потенциал для тематического моделирования на коротких текстах, однако методы, основанные на более сложных внутренних представлениях текста, демонстрируют более высокое качество путем лучшей репрезентации близости между текстами и результирующими темами.

## Заключение
Мы сравнили три инструмента для тематического моделирования: иерархическое стохастическое блокмоделирование (SBMTM), Latent Dirichlet Allocation с эмбеддингами слов Navec и подход на основе трансформер-модели (BERTopic) - и пришли к выводу, что последний значительно превосходит свои альтернативы на нашем корпусе. Тем не менее, несмотря на вычислительное преимущество, BERTopic может быть нежеланным инструментом для исследователя ввиду того, что производимые им темы крайне локальны и не позволяют выделять более генерализуемые направления в дискурсе. SBMTM не страдает от этой проблемы и производит более генерализуемые темы, однако производит больше тем с низкой когерентностью.