## Современные методы анализа неструктурированной текстовой информации: систематизация и сравнительный анализ

### Введение
Цель настоящей главы и приведенного в ней аналитического обзора состоит в систематизации различных современных подходов к текстовому анализу на основе нейросетевых моделей и их сравнительном анализе для оценки эффективности применения для прикладных задач. Достижение этой цели требует решения следующих задач:

1. Рассмотреть ключевые прикладные задачи текстового анализа.
2. Проанализировать энкодер и декодер модели

Современные методы анализа неструктурированной информации решают прикладные задачи обработки текстов, включающие в себя следующие задачи:

- Извлечение именованных сущностей (Named Entity Recognition, NER). Это задача выделения и классификации именованных сущностей, таких как имена, места, даты, организации и другие в тексте.
- Извлечение отношений (Relation Extraction). Задача определения связей между различными именованными сущностями в тексте, например, определение связи между человеком и местом работы.
- Перефразирование (Paraphrase Generation). Генерация семантически эквивалентных предложений или выражений с целью переформулирования их с сохранением смысла.
- Суммаризация (Summarization). Автоматическое создание кратких сводок из больших объемов текста. Суммаризация может выполняться как по одному тексту, так и по массиву текстов.
- Ответы на вопросы (Question Answering, Q&A). Построение моделей, способных отвечать на вопросы, заданные в естественном языке.
- Классификация текста (Text Classification). Разделение текста на категории или классы на основе его содержания или тематики.
- Кластеризация текста (Text Clustering). Структурирование массива текстовых документов на основе семантической близости их содержания без знания изначальной структуры массива.
- Определение частей речи (Part-of-Speech Tagging, POS). Присвоение токенам в тексте соответствующих частей речи, таких как существительные, глаголы, прилагательные и др.
- Машинный перевод (Machine Translation). Автоматический перевод текста с одного языка на другой с сохранением смысла.
- Разрешение семантических ссылок (Coreference Resolution). Определение, к каким сущностям или объектам в тексте относятся местоимения или фразы.

Современные модели анализа неструктурированной информации базируются на представленной в 2017 году статье "Attention is All You Need" [@vaswani2023]. Эта архитектура нашла широкое применение благодаря своей способности эффективно работать с последовательностями большой длины без использования рекуррентных или свёрточных слоев.

### Основные компоненты модели трансформер
1. Encoder-Decoder архитектура
Трансформер состоит из двух основных частей: энкодера и декодера. Энкодер принимает на вход последовательность слов и преобразует её в скрытые представления. Декодер занимается генерацией текста, используя эти скрытые представления энкодера для предсказания следующих слов.

2. Механизм внимания (Self-Attention)
Одной из ключевых концепций трансформера является механизм внимания, позволяющий модели эффективно работать с контекстом. Этот механизм позволяет модели "фокусироваться" на различных частях входной последовательности при генерации выходных предсказаний. Self-Attention позволяет модели учитывать дальние зависимости в тексте, что делает её способной лучше понимать контекст.

3. Позиционное кодирование
Трансформер обрабатывает всю последовательность токенов целиком, в отличие от рекуррентных сетей, обрабатывающих последовательность токенов один за другим. Для сохранения информации о позициях слов в последовательности в трансформере используется позиционная кодировка. Это позволяет модели учитывать порядок слов при реализации механизма внимания.

4. Многоуровневые слои
Трансформер состоит из множества слоев энкодера и декодера. Каждый слой состоит из нескольких подслоёв: механизм внимания, полносвязные нейронные сети и нормализация. Многие такие слои обеспечивают более глубокое представление для текста.

Благодаря возможности обрабатывать всю последовательность целиком Трансформер позволяет эффективно значительно повысить количество текстов, обрабатываемое за единицу времени, что ускоряет процесс обучения. Благодаря механизму внимания трансформер лучше улавливает дальние зависимости в тексте.

Модель трансформер показывает высокую гибкость и применима к различным задачам NLP без необходимости переобучения модели с нуля. На основе модели трансформер сделано большинство современных языковых моделей, разделившихся на генеративные и энкодерные. Эти модели стали краеугольными камнями в различных задачах, таких как классификация текстов, извлечение сущностей, определение тональности и кластеризация данных.
### Генеративные модели
Генеративные модели, такие как рекуррентные нейронные сети (RNN) и свёрточные нейронные сети (CNN), создают данные, имитируя вероятностные распределения слов или символов в тексте. Примерами таких моделей являются LSTM (Long Short-Term Memory) и GRU (Gated Recurrent Unit), способные учитывать контекст и последовательность слов при генерации текста. Однако, генеративные модели могут страдать от проблемы затухающего или взрывающегося градиента и могут иметь ограниченные возможности в генерации длинных текстов.

### Энкодерные модели
Энкодерные модели, такие как Transformer и его вариации (например, BERT, GPT), работают как механизмы кодирования и понимания текста. Они преобразуют входную данные в векторные представления с помощью механизмов внимания и многоуровневых архитектур. Энкодерные модели, обученные на больших объемах данных, обычно демонстрируют высокую производительность в решении задач NLP.

С непрерывным развитием технологий глубокого обучения в последние годы, предварительно обученные языковые модели Pre-trained Language Models (PLM), которые обучаются с использованием самообучения на огромных корпусах текста, широко используются в области обработки естественного языка (NLP) и показывают лучшие результаты в различных задачах обработки текста. В отличие от традиционного обучения с учителем, PLM, основанные на самообучении, предварительно обучаются на обширных неразмеченных текстах, а затем дообучаются для решения конкретных задач на маломасштабных размеченных данных.

### Сравнение моделей для задач NLP
Классификация текстов
При выполнении задачи классификации текстов, где необходимо отнести текст к определенной категории, энкодерные модели, такие как BERT, показывают высокую точность благодаря способности улавливать семантическую информацию и контекст предложений.

#### Извлечение сущностей
В задаче извлечения сущностей, где требуется определить и классифицировать именованные сущности в тексте (такие как имена, места, даты и т.д.), энкодерные модели, такие как BERT и GPT, успешно применяют механизмы внимания для извлечения информации из текста.

#### Определение тональности
Для определения тональности текста, где необходимо понять отношение текста к определенной эмоциональной окраске (позитивной, негативной, нейтральной), как генеративные, так и энкодерные модели могут показать высокую точность. Однако, модели, основанные на энкодерах, обычно имеют более широкий спектр понимания контекста, что делает их более эффективными в данной задаче.

#### Кластеризация данных
В задаче кластеризации, где необходимо группировать похожие текстовые данные, энкодерные модели, такие как BERT, имеют тенденцию лучше улавливать семантические связи и создавать более качественные кластеры.

### Выводы
Генеративные и энкодерные языковые модели имеют свои преимущества и недостатки в решении различных задач NLP. Выбор модели зависит от конкретной задачи, доступных ресурсов и требуемой точности. В последнее время энкодерные модели, особенно на основе трансформеров, демонстрируют значительные успехи благодаря своей способности к обучению на больших корпусах текста и высокой гибкости в различных задачах NLP.

